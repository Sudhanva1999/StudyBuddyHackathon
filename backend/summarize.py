import nltk
from sentence_transformers import SentenceTransformer, util
from transformers import T5ForConditionalGeneration, T5Tokenizer

nltk.download('punkt')
nltk.download('punkt_tab')
model = SentenceTransformer('all-MiniLM-L6-v2')
tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)
t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')


def paraphrase(sentence):
    input_text = f"paraphrase: {sentence.strip()}"
    input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=256, truncation=True)

    outputs = t5_model.generate(
        input_ids,
        max_length=128,
        num_beams=5,
        early_stopping=True,
        do_sample=True,
        temperature=0.5,
        top_k=40,
        top_p=0.9,
        repetition_penalty=1.2
    )

    if outputs is None or len(outputs) == 0:
        return sentence

    result = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
    result = result.replace("Paraphrase:", "").strip()

    if result.lower() == "true" or len(result.split()) <= 1:
        return sentence

    return result


def generate_summary(title, document, num_sentences=5):
    sentences = nltk.sent_tokenize(document)

    if len(sentences) == 0:
        return "No valid sentences to summarize."

    title_embedding = model.encode(title, convert_to_tensor=True)
    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)

    similarity_scores = util.pytorch_cos_sim(title_embedding, sentence_embeddings)[0]

    ranked_sentences = sorted(
        [(score.item(), sentence) for score, sentence in zip(similarity_scores, sentences)],
        key=lambda x: x[0],
        reverse=True
    )

    selected_sentences = [sentence for i, sentence in ranked_sentences[:num_sentences]]
    paraphrased_sentences = [paraphrase(sentence) for sentence in selected_sentences]

    print("\n=== Paraphrased Sentences ===")
    for idx, sent in enumerate(paraphrased_sentences, 1):
        print(f"{idx}. {sent}")

    if paraphrased_sentences:
        input_text = "summarize: " + " ".join(paraphrased_sentences)
        input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=2048, truncation=True)

        outputs = t5_model.generate(
            input_ids,
            max_length=1024,
            num_beams=4,
            early_stopping=False,
            do_sample=True
        )
        summary = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

        print("\n=== Generated Summary ===")
        print(summary)
    else:
        summary = "Summary generation failed due to insufficient data."

    return summary

title = "Overview of Reinforcement Learning Approaches and Techniques"
document = """
Welcome back. So I've started this video lecture series on reinforcement learning. And the last three videos were at a very high level kind of what is reinforcement learning, how does it work, what are some of the applications. But we really didn't dig into too many details on the actual algorithms of how you implement reinforcement learning in practice. And so that's what I'm actually going to do today and in this next part of this series is something I hope is going to be really, really useful kind of for all of you, which is the first thing is I'm going to kind of organize the different approaches of reinforcement learning. So this is a massive field that's about a hundred years old, right, this merges neuroscience, behavioral science, like Pavlov's dog, optimization theory, optimal control, I think, Bellman's equation and the Hamilton Jacobi Bellman equation, all the way to modern day, deep reinforcement learning, which is kind of how to use powerful machine learning techniques to solve these optimization problems. And you'll remember that in my view of reinforcement learning, this is really at the intersection of machine learning and control theory. So we're essentially machine learning good effective control strategies to interact with in environment. Okay, good. And so in this first lecture, what I'm going to do, and I think I'm hoping that this is actually super useful for some of you, is I'm going to talk through the organization of these different decisions you have to make and kind of how you can think about the landscape of reinforcement learning. Before going on, I want to mention, this is actually a chapter in the new second edition of our book, Data-Durven Science and Engineering, with myself and Nathan Kutz. And reinforcement learning was one of the new chapters I decided to write. So this is a great excuse for me to get to learn more about reinforcement learning. And it's also a nice opportunity for me to kind of get to communicate more details to you. So if you want to download this chapter, the link is here. I'll also put it in the comments below. And I'll have a link to the second edition of the book up soon as well, probably in the comments. Good. So a new chapter you can follow along with all of the videos and each video kind of follows the chapter. Good. So before I get into that organizational chart of how all of these different types of reinforcement learning can be thought of, I want to just do a really, really quick recap of what is the reinforcement learning problem. So in reinforcement learning, you have an agent that gets to interact with the world or the environment through a set of actions. Sometimes these are discrete actions. Sometimes they're continuous actions. If I have a robot, I might have a continuous action space. Whereas if I'm playing a game, if I'm the white pieces on a chess board, then I have a discrete set of actions, even though it might be kind of high dimensional. And I observe the state of the system at each time step. I get to observe the state of the system and use that information to change my actions to try to maximize my current or future rewards through playing. And I'll mention that in lots of applications, for example, in chess, the reward structure might be quite sparse. I might not get any feedback on whether or not I'm making good moves until the very end when I either win or lose. Tic Tac Toe, Back Amin, Checkers, Go are all kind of the same way. And that delayed reward structure is one of the things that makes this reinforcement learning problem really, really challenging. It's what makes learning in animal systems also challenging. If you want to teach your dog a trick, they have to know kind of step by step what you want them to do. And so you actually sometimes have to give them rewards at intermediate steps to train a behavior. And so the agent, their control strategy or their policy is typically called pi. And it basically is a probability of taking action a given a state s, a current state s. And this could be a deterministic policy. It could be a probabilistic policy. But essentially, it's a set of rules that determines what actions I as the agent take, given what I sense in the environment to maximize my future rewards. So that's the policy. And again, usually this is written in a probabilistic framework because typically the environment is written as a probabilistic model. And there is something called a value function. So given, you know, some policy pi that I take, then I can associate a value with being in each of the states of the system, essentially by what is my expected future reward add up all of my future rewards. What's the expectation of that? And we'll put in this little discount factor because future rewards might be less advantageous to me than than current rewards. This is just something that people do in economic theory. It's kind of like a utility function. And so for every policy pi, there is a value associated with being in each of the given states s. Now again, I'll point out for even reasonably sophisticated problems. You can do this for tic-tac-toe. You can enumerate all possible states and all possible actions. And you can compute this value function kind of through brute force. But even for moderately complicated games, even like checkers, let alone backgammon or chess or go, this state space, the space of all possible states you could observe your system in, is astronomically large. I think it's estimated that there's 10 to the 80 plus, maybe it's 10 to the 180, it's a huge number of possible chess boards, even more possible go boards. And so you can't really actually enumerate this value function. But it's a good kind of abstract function that we can think about, or these policy functions and these value functions. And at least in simplistic dynamic programming, we often assume that we know a model of our environment. And I'll get to that in a minute. So the goal here, the entire goal of reinforcement learning is to learn through trial and error, through experience, what is the optimal policy to maximize your future rewards. So notice that this value function is a function of policy. Pi, I want to learn the best possible policy that always gives me the most value out of every board position, out of every state. And that's a really hard problem. It's easy to state the goal. And it is really, really hard to solve this problem. That's why it's been this growing field for 100 years. That's why it's still a growing field because we have more powerful emerging techniques in machine learning to start to solve this problem. So that's the framework. I need you to know what the policy is. That is the set of kind of rules or controllers that I as an agent get to take to manipulate my environment. The value function tells me how valuable it is to be in a particular state. So I might want to move myself into that state. So I need you to know kind of this nomenclature. So now I can kind of show you how all of these techniques are organized. Okay, so that's what I'm going to do for the rest of the video is we're going to talk through kind of the key organization of all of the different like mainstream types of reinforcement learning. Okay, so the first biggest dichotomy is between model based and model free reinforcement learning. So if you actually have a good model of your environment, you have some, you know, Markov decision process or some differential equation. If you have a good model to start with, then you can work in this model based reinforcement learning world. Now some people don't actually consider what I'm about to talk about reinforcement learning, but I do. Okay, so for example, if my environment is a Markov decision process, which means that there is a probability kind of a deterministic probability that sounds like an oxymoron. But if there is a specified probability of moving from state s to the next state s prime given action a. And this probability function is known. So it's a, it doesn't depend on the history of your actions and states. It only depends on the current state and the current action determines a probability of going to a next state s prime. Then two really, really powerful techniques that I'm going to tell you about to optimize the policy function pi is policy iteration and value iteration. So these allow you to essentially iteratively walk through the game or the Markov decision process, taking actions that you think are going to be the best, and then assessing what the value of that action and state actually are, and then kind of refining and iterating the policy function and the value function. So that is a really, really powerful approach if you have a model of your system, you can run this kind of on a computer and, and kind of determine learn what the best policy and value is. And this is kind of a special case of dynamic programming that relies on the Bellman optimality condition for the value function. So I'm going to do a whole lecture on this blue part right here. We're going to talk about policy iteration and value iteration and how they are essentially dynamic using dynamic programming on the value function, which satisfies Bellman's optimality condition. Now that was for probabilistic processes, things where maybe you know, like backgammon where there's a dice roll at every turn for deterministic systems like a robot system or a self driving car. So I'm going to do a human, you know, my reinforcement learning problem. This is much more of a continuous control problem. In which case I might have some nonlinear differential equation x dot equals f of x comma u. And so the linear optimal control that we studied, I guess in chapter eight of the book. So linear quadratic regulators, common filters, things like that optimal linear control problems. So there are some special cases of this optimal nonlinear control problem with the Hamilton Jacobi Bellman equation. Again, this relies on Bellman optimality. And you can use kind of dynamic programming ideas to solve optimal nonlinear control problems like this. Now I'll point out mathematically this is a beautiful theory. It's powerful. And for decades and it's kind of the textbook way of thinking about how to design optimal policies and optimal controllers, you know, for Markov decision processes and for nonlinear control systems. In practice, actually solving these things with dynamic programming ends up usually amounting to a brute force search. And it's usually not scalable to high dimensional systems. Typically it's hard to do this optimal Hamilton Jacobi Bellman type nonlinear control for an even moderately high dimensional system. You know, you can do this for a three dimensional system. Sometimes a five dimensional system. Maybe, you know, I've heard special cases with machine learning. You can do this maybe for a 10 or 100 dimensional system. But you can't do this for the nonlinear fluid flow equations, which might have, you know, 100,000 or a million dimensional differential equation when you write it down on your computer. So important caveat there. But that's model based control. And a lot of what we're going to do in model free control uses ideas that we learned from model based control. So even though, you know, I don't actually do a lot of this in my daily life with reinforcement learning. Most of the time we don't have a good model of our system. For example, in chess, I don't have a model of my opponent, for example, or at least I can't write it down mathematically as a Markov decision process. So I can't really use these techniques, but a lot of what model free control reinforcement learning is going to do is kind of approximate dynamic programming where you're simultaneously learning kind of the dynamics or learning to update these these functions through trial and error without actually having a model. And so in model free reinforcement learning, kind of the major dichotomy here is between gradient free and gradient based methods. And I'll tell you what this means in a little bit. But for example, if I can parameterize my policy pie by some variables theta. And I know kind of what the dependency with those variables theta are, I might be able to take the gradient of my reward function or my value function with respect to those parameters directly and speed up the optimization. So gradient based, if you if you can use it is usually going to be the fastest, most efficient way to do things. But oftentimes, again, we don't have gradient information. We're just playing games. We're playing chess. We're playing go. And I can't compute the derivative of one game with respect to another. That's hard for me at least to do. And so within gradient free, there's a lot of dichotomy here. There's a dichotomy of a dichotomy of an dichotomy within gradient free control. There is this idea of sometimes you can be off policy or on policy. And it's a really important distinction. What on policy means is that let's say I'm playing a bunch of games of chess. I'm trying to learn an optimal policy function or an optimal value function or both by playing games of chess and iteratively kind of refining my estimate of pi or a V. What on policy means is that I always play my best game possible. Whatever I think the value function is and whatever I think my best policy possible is, I'm always going to use that best policy as I play my game. And I'm going to always try to kind of get the most reward out of my system every game I play. That's what it means to be on policy. Off policy means, well, maybe I'll try some things. Maybe I know that my policy is suboptimal. And so I'm just going to do some random moves occasionally. That is called off policy because I think they're suboptimal, but they might be really valuable for learning information about the system. So on policy methods include this Sarsa state action reward state action. And there's all of these variants of the Sarsa algorithm, this on policy reinforcement learning. And these TDs mean temporal difference. And MC is Monte Carlo. And so there's this whole family of kind of gradient free optimization techniques that use different kind of amounts of history. I'll talk all about that. That's going to be a whole other lecture is this red box gradient free model free reinforcement learning. And so the off policy version of Sarsa, kind of this on policy set of algorithms, there is an off policy variant called Q learning. And this quality function Q is kind of the joint value, if you like, of being in a particular state and taking a particular action a. So this quality function contains all of the information of my optimal policy and the value function. And both of these can be derived from the quality function. But the really important distinction is that when we learn based on the quality function, we don't need a model for what my next state is going to be. This quality function kind of implicitly defines the value of, you know, based on where you're going to go in the future. And so Q learning is a really nice way of learning when you have no model. And you can take off policy information and learn from that. You can take a suboptimal controller just to see what happens and still learn and get better policies and better value functions in the future. And that's also really important if you want to do imitation learning, if I want to just watch other people play games of chess, even though I don't know what their value function is or what their policy is. With these off policy learning algorithms, you can accumulate that information into your estimate of the world. And every bit of information you get improves your quality function and it improves the next game you're going to play. So really powerful. And I would say most of what we do nowadays, you know, is kind of in this Q learning world. A lot, a lot, a lot of machine learning is Q learning reinforcement learning is Q learning. And then the gradient based algorithms, I'm not going to talk about it too much here, but it's essentially where you would actually update the parameters of your policy or your value function or your Q function directly using some kind of a gradient optimization. So if I can sum up all of my future rewards, and it's a function of the current parameters theta that parameterize my policy, then I might be able to use gradient optimization, things like Newton steps and steepest descent, things like that to get a good estimate. And this, when I have the ability to do that is going to be way, way faster than any of these, these gradient free methods and even in terms, will be faster than dynamic programming. And so the last piece of this is kind of in the last 10 years, we've had this massive explosion of deep reinforcement learning. A lot of this has been because of deep mind and alpha go, you know, demonstrating that machines, computers can play Atari games at human level performance, they can beat grandmasters that go just incredibly impressive demonstrations of reinforcement learning that now use deep neural networks. Either to learn a model where you can then use model based reinforcement learning or to represent these kind of model free concepts. So you can have like a deep neural network for the quality function. You can have a deep neural network for the policy and then differentiate with respect to those network parameters using kind of, you know, auto diff and back propagation to do gradient based optimization on your policy network. I would say that deep model predictive control, this doesn't exactly fit into the reinforcement learning world, but I would say, you know, it's, it's morally very closely related. Deep model predictive control allows you to solve these kind of hard optimal nonlinear problems. And then you can actually learn a policy based on what your model predictive controller actually does. You can essentially kind of codify that model predictive controller into a control policy. And finally, actor critic methods, I'm, actor critic methods existed long before deeper reinforcement learning, but nowadays they have kind of a renewed interest because you can, you can, you can train these with with deep neural networks. Okay, so that is the mile high view as I see it. The different categories of reinforcement learning is this comprehensive, absolutely not. Is it 100% factually correct? Definitely not. This is, you know, a rough sketch of the main divides and things you need to think about when you're choosing to reinforcement learning algorithm. If you have a model of your system, you can use dynamic programming based on Bellman optimality. If you don't have a model of the system, you can either use gradient free or gradient based methods. And then there's on policy and off policy variants depending on, you know, your specific needs tends to be that Sarsa methods are more conservative and q learning will tend to converge faster. And then for all of these methods, there are ways of kind of making them more powerful and more flexible representations using deep neural networks in kind of different focused ways. Okay, so in the next few videos, we'll zoom into, you know, this part here from archive decision processes, how we do policy iteration and value iteration will actually derive the quality function here. We'll talk about model free control, these kind of gradient free methods on policy and off policy. Q learning is one of the most important ones and temporal difference learning actually has a lot of neuro science analogs, so how we learn in our animal brains. People think, you know, is very closely related to these TD learning policies. We'll talk about how you do optimal nonlinear control with the Hamilton Jacobi Bellman equation. We'll talk very briefly about policy gradient optimization and then, you know, all of these, there are kind of deep learning things. We'll pepper it throughout with deep learning or maybe I'll have a whole lecture on these deep learning methods. So that's all coming up really excited to walk you through this. Thank you.
"""

# summary = generate_summary(title, document)
